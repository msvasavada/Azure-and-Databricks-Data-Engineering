{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec0056f-8873-46a0-b546-cf6bec8ea80d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fada5e-d648-41f4-8b04-1b727917d66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.5 Lab - Create and Execute Unit Tests\n",
    "\n",
    "### Estimated Duration: 15-20 minutes\n",
    "\n",
    "By the end of this lab, you will have practiced creating and executing unit tests for the modularized functions that were created in the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdef4194-f1cb-4517-aa24-b97049487755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf8e01d-145a-4d6a-96bf-e1b8beca2777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. View the Functions in the Python File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0db35fe-3d9b-4ace-8385-16c29fbd226c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. From the **./Course Notebooks/M02 - CI** folder, navigate to the file **[./src_lab/lab_functions/transforms.py]($./src_lab/lab_functions/transforms.py)**. This Python file contains the modularized functions from the previous lab. \n",
    "\n",
    "    Confirm that the file contains the `convert_miles_to_km` and `uppercase_column_names` functions.\n",
    "\n",
    "\n",
    "**Code in the transforms.py file:**\n",
    "```\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def convert_miles_to_km(df, new_column_name, miles_column):\n",
    "    return df.withColumn(new_column_name, F.round(F.col(miles_column) * 1.60934, 2))\n",
    "\n",
    "\n",
    "def uppercase_columns_names(df):\n",
    "    return df.select([F.col(col).alias(col.upper()) for col in df.columns])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf37171-e8c4-47c7-bf36-3d909e0013f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create Unit Tests\n",
    "\n",
    "Create two unit tests, one for each of the functions in the file above. \n",
    "\n",
    "It's typically easier to develop the unit tests within the notebook (or locally) and then move them to a separate **.py** file later to execute them with `pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9743fc-bded-4fad-9d73-14b576e97ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Start by importing the `transforms` functions from the `lab_functions` module located in the `src_lab` directory, making them available for use in the current notebook.\n",
    "\n",
    "**HINT:** The **src_lab** folder is in the same directory as this notebook. You don't have to use `sys.path.append()` to append the python path. The current path is appended by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053d2c19-1cf8-4211-be4c-984e2d63a709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src_lab.lab_functions import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a9b8d7-6f5f-4176-825d-68ef4d3f43e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the unit test `test_uppercase_columns_function` function to test the custom `transforms.uppercase_column_names()` function. \n",
    "\n",
    "    Use the starter code below to help guide you. After you are done, run the unit test function and confirm that it does not return an error.\n",
    "\n",
    "**NOTE:** There are a variety of ways to test this function to. We will keep it simple for this lab.\n",
    "\n",
    "**SOLUTION:** Solution can be found in the **[./tests_lab/lab_unit_test_solution.py]($./tests_lab/lab_unit_test_solution.py)** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21160d6f-444b-4ddc-85d0-daf6f4f7a49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "def test_uppercase_columns_function():\n",
    "\n",
    "    ## Fake DataFrame with random column names\n",
    "    data = [(1, 5.0, 1, 1, 1, 1)]\n",
    "    columns = [\"id\", \"trip_distance\", \"My_Column\", \"WithNumbers123\", \"WithSymbolX@#\", \"With Space\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    ## Apply the transforms.uppercase_columns_names function to return the actual column names\n",
    "    actual_df = transforms.uppercase_columns_names(df)\n",
    "    actual_columns = actual_df.columns\n",
    "\n",
    "    ## Create a list of the expected column names\n",
    "    expected_columns = ['ID', 'TRIP_DISTANCE', 'MY_COLUMN', 'WITHNUMBERS123', 'WITHSYMBOLX@#', \"WITH SPACE\"]\n",
    "\n",
    "    ## Perform a test of the actual columns names and expected column names using a simple python assert statement\n",
    "    assert actual_columns == expected_columns\n",
    "    print('Test Passed!')\n",
    "\n",
    "test_uppercase_columns_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89f42b5-ce66-4552-bcc3-48a15b59ceae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the unit test `test_convert_miles_to_km_function` to test the custom `transforms.convert_milles_to_km` function. Use the `pyspark.testing.utils.assertDataFrameEqual` function to test the actual DataFrame against the expected DataFrame.\n",
    "\n",
    "    Use the starter code below to help guide you. After you are done, run the unit tests and confirm that it does not return an error.\n",
    "\n",
    "**NOTE:** There are a variety of unit tests you can run on the function. This is a simple example that tests the function on positive and null values. We should also test this function on negative values, but we will ignore those for this lab for simplicity.\n",
    "\n",
    "**HINT:** [pyspark.testing.assertDataFrameEqual](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.testing.assertDataFrameEqual.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e53182e-5ea1-41b2-8270-408304638101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "\n",
    "def test_convert_miles_to_km_function():\n",
    "    # Prepare a DataFrame with sample data\n",
    "    data = [(1.0,), (5.5,), (None,)]\n",
    "    schema = StructType([\n",
    "        StructField(\"trip_distance_miles\", DoubleType(), True)  # Allow null values by setting nullable=True\n",
    "    ])\n",
    "    actual_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "    ## Apply the function on the sample data and store the actual DataFrame\n",
    "    actual_df = transforms.convert_miles_to_km(df = actual_df, \n",
    "                                               new_column_name=\"trip_distance_km\",   ## Name of the new column\n",
    "                                               miles_column=\"trip_distance_miles\")   ## Name of the source miles column\n",
    "\n",
    "\n",
    "    ## Create an expected DataFrame with a defined schema using StructField DoubleType for each column\n",
    "    data = [\n",
    "        (1.0, 1.61),   # Row with values\n",
    "        (5.5, 8.85),   # Row with values\n",
    "        (None, None) # Row with null values\n",
    "    ]\n",
    "\n",
    "    ## Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"trip_distance_miles\", DoubleType(), True),\n",
    "        StructField(\"trip_distance_km\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    ## Create expected DataFrame\n",
    "    expected_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "    ## Compare the actual and expected DataFrames using assertDataFrameEqual\n",
    "    assertDataFrameEqual(actual_df, expected_df)\n",
    "    print('Test Passed!')\n",
    "\n",
    "\n",
    "## Run the unit test\n",
    "test_convert_miles_to_km_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30f4a97-32f0-496d-a660-185bc1647760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Use `pytest` to Execute the Unit Tests\n",
    "\n",
    "Next, use `pytest` to execute the unit tests. For this portion of the lab, you can do one of the following:\n",
    "\n",
    "**C1. DURING A LIVE CLASS**\n",
    "- Use `pytest` to execute the unit tests in the solution Python file that is already provided for you: **./tests_lab/lab_unit_test_solution.py**.\n",
    "\n",
    "**C2. CHALLENGE (COMPLETE AFTER CLASS)**\n",
    "- Migrate your unit tests from above into a new **your-file-name.py** file in the **tests_lab/** folder, and then use `pytest` to execute your file. Make sure to add your `pytest` fixture to create a Spark session and import the necessary packages to run the unit tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04661e4-e0cb-4351-8fe5-20f9b219c2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Import the `pytest` package version 8.3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea1ca93-f745-4131-81f5-34f25617945b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest==8.3.4\n  Obtaining dependency information for pytest==8.3.4 from https://files.pythonhosted.org/packages/11/92/76a1c94d3afee238333bc0a42b82935dd8f9cf8ce9e336ff87ee14d9e1cf/pytest-8.3.4-py3-none-any.whl.metadata\n  Using cached pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\nCollecting iniconfig (from pytest==8.3.4)\n  Obtaining dependency information for iniconfig from https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl.metadata\n  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from pytest==8.3.4) (23.2)\nCollecting pluggy<2,>=1.5 (from pytest==8.3.4)\n  Obtaining dependency information for pluggy<2,>=1.5 from https://files.pythonhosted.org/packages/54/20/4d324d65cc6d9205fabedc306948156824eb9f0ee1633355a8f7ec5c66bf/pluggy-1.6.0-py3-none-any.whl.metadata\n  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\nUsing cached pytest-8.3.4-py3-none-any.whl (343 kB)\nUsing cached pluggy-1.6.0-py3-none-any.whl (20 kB)\nUsing cached iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: pluggy, iniconfig, pytest\nSuccessfully installed iniconfig-2.1.0 pluggy-1.6.0 pytest-8.3.4\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest==8.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e46b11-15c8-44fa-9c23-e7f920ecb993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "2. If you are creating your own **.py** file for the challenge you can enable the autoreload extension to reload any imported modules automatically so that the command runs pick up those updates as you make them in the .py file. \n",
    "\n",
    "    Use the following commands in any notebook cell or Python file to enable the autoreload extension.\n",
    "\n",
    "    Documentation: [Autoreload for Python modules](https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47cd6c6-b61b-440f-a54e-ca2dfea93a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b168a5-a893-4e71-a919-eb39260cb385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Execute `pytest` on the **./tests_lab/lab_unit_test_solution.py** file. Run the cell and confirm both unit tests pass.\n",
    "\n",
    "**NOTE:** If you are completing the challenge, modify the path to test your specific **.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2046f158-b945-4fbd-9330-3fff7c4645c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts ==============================\u001B[0m\nplatform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.6.0 -- /local_disk0/.ephemeral_nfs/envs/pythonEnv-4689f5dc-32bc-4a42-9c3e-d36098b3d3f1/bin/python\nrootdir: /Workspace/Users/labuser10685686_1750443211@vocareum.com/devops-essentials-for-data-engineering-2.0.4/DevOps Essentials for Data Engineering\nconfigfile: pytest.ini\n\u001B[1mcollecting ... \u001B[0mcollected 2 items\n\ntests_lab/lab_unit_test_solution.py::test_uppercase_columns_function \u001B[32mPASSED\u001B[0m\u001B[32m [ 50%]\u001B[0m\ntests_lab/lab_unit_test_solution.py::test_convert_miles_to_km_function \u001B[32mPASSED\u001B[0m\u001B[32m [100%]\u001B[0m\n\n\u001B[32m============================== \u001B[32m\u001B[1m2 passed\u001B[0m\u001B[32m in 4.67s\u001B[0m\u001B[32m ===============================\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import sys\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "retcode = pytest.main([\"./tests_lab/lab_unit_test_solution.py\", \"-v\", \"-p\", \"no:cacheprovider\"])\n",
    "\n",
    "assert retcode == 0, \"The pytest invocation failed. See the log for details.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6890b79d-4c6e-4b18-b9d9-391bbbef9dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.5L Solution - Create and Execute Unit Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}